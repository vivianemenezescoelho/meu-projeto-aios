# Tool Evaluation Framework

**Version:** 2.0
**Purpose:** Framework de avaliaÃ§Ã£o de ferramentas baseado em **anÃ¡lise comparativa** e **tiers relativos**, nÃ£o em valores absolutos arbitrÃ¡rios.

## Filosofia Central

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PRINCÃPIO FUNDAMENTAL                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  NÃƒO EXISTEM VALORES ABSOLUTOS UNIVERSAIS.                      â”‚
â”‚                                                                 â”‚
â”‚  Um projeto com 30 stars pode ser EXCELENTE se for o melhor     â”‚
â”‚  do nicho. Um projeto com 10,000 stars pode ser MEDIANO se      â”‚
â”‚  todos os concorrentes tiverem 50,000.                          â”‚
â”‚                                                                 â”‚
â”‚  SEMPRE comparar ferramentas ENTRE SI, dentro do contexto       â”‚
â”‚  da pesquisa realizada.                                         â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### PrincÃ­pios Operacionais

1. **ComparaÃ§Ã£o Relativa** - Tiers baseados nos projetos encontrados, nÃ£o em nÃºmeros mÃ¡gicos
2. **Contexto de DomÃ­nio** - Ferramentas de nicho competem com ferramentas de nicho
3. **SeguranÃ§a Ã© Factual** - CVE crÃ­tica Ã© CVE crÃ­tica (Ãºnico absoluto aceitÃ¡vel)
4. **Nenhum VETO Prematuro** - Projetos pequenos podem ser a melhor opÃ§Ã£o
5. **Dados > HeurÃ­sticas** - Normalizar dentro do dataset encontrado

---

## 1. METODOLOGIA DE TIERS RELATIVOS

### 1.1 Como Funciona

```
PASSO 1: Pesquisa encontra N ferramentas para o domÃ­nio
         Exemplo: 25 MCPs para "copywriting"

PASSO 2: Para CADA dimensÃ£o, coletar valores brutos
         stars: [12, 45, 89, 150, 500, 2000, ...]
         downloads: [100, 500, 2000, 10000, ...]

PASSO 3: Calcular percentis DENTRO do conjunto
         P20 = valor no percentil 20
         P50 = mediana
         P80 = valor no percentil 80

PASSO 4: Classificar cada ferramenta por tier RELATIVO
         Tier 1: Top 20% (acima de P80)
         Tier 2: 21-50% (entre P50 e P80)
         Tier 3: 51-80% (entre P20 e P50)
         Tier 4: Bottom 20% (abaixo de P20)

PASSO 5: Combinar tiers de mÃºltiplas dimensÃµes
         Tool X: Stars=Tier1, Downloads=Tier2, Activity=Tier1
         â†’ Composite Tier = weighted average
```

### 1.2 CÃ¡lculo de Percentis

```python
def calculate_tier(value, all_values):
    """
    Retorna tier 1-4 baseado na posiÃ§Ã£o relativa do valor
    dentro do conjunto de todos os valores encontrados.
    """
    sorted_values = sorted(all_values)
    n = len(sorted_values)

    p20 = sorted_values[int(n * 0.20)]
    p50 = sorted_values[int(n * 0.50)]
    p80 = sorted_values[int(n * 0.80)]

    if value >= p80:
        return 1  # Top 20%
    elif value >= p50:
        return 2  # 21-50%
    elif value >= p20:
        return 3  # 51-80%
    else:
        return 4  # Bottom 20%
```

### 1.3 NormalizaÃ§Ã£o de Score (0-10)

```python
def normalize_score(value, all_values):
    """
    Normaliza valor para escala 0-10 baseado no range encontrado.
    Usa log scale para mÃ©tricas com distribuiÃ§Ã£o exponencial (stars, downloads).
    """
    import math

    # Log scale para mÃ©tricas exponenciais
    log_value = math.log10(value + 1)
    log_min = math.log10(min(all_values) + 1)
    log_max = math.log10(max(all_values) + 1)

    # Normalizar para 0-10
    if log_max == log_min:
        return 5.0  # Todos iguais

    normalized = ((log_value - log_min) / (log_max - log_min)) * 10
    return round(normalized, 2)
```

---

## 2. DIMENSÃ•ES DE AVALIAÃ‡ÃƒO

### 2.1 Prova Social (Social Proof)

**MÃ©tricas coletadas (valores brutos):**

| MÃ©trica | Fonte | Tipo |
|---------|-------|------|
| `github_stars` | GitHub API | Exponencial |
| `github_forks` | GitHub API | Exponencial |
| `npm_downloads_weekly` | npm API | Exponencial |
| `pypi_downloads_monthly` | PyPI API | Exponencial |
| `contributors_count` | GitHub API | Linear |
| `open_issues_count` | GitHub API | Linear |
| `closed_issues_count` | GitHub API | Linear |
| `last_commit_days_ago` | GitHub API | Inverso (menor = melhor) |
| `releases_last_year` | GitHub API | Linear |
| `stackoverflow_questions` | SO API | Exponencial |

**CÃ¡lculo do Tier de Social Proof:**

```yaml
social_proof_tier:
  components:
    - dimension: popularity
      metrics: [github_stars, npm_downloads, pypi_downloads]
      weight: 0.30

    - dimension: community_health
      metrics: [contributors_count, issues_response_ratio]
      weight: 0.25

    - dimension: activity
      metrics: [last_commit_days_ago, releases_last_year]
      weight: 0.25
      note: "last_commit Ã© INVERSO - menor = melhor"

    - dimension: adoption
      metrics: [stackoverflow_questions, known_users]
      weight: 0.20

  calculation: |
    Para cada dimension:
      1. Normalizar cada mÃ©trica (0-10) dentro do dataset
      2. MÃ©dia das mÃ©tricas da dimension
      3. Aplicar weight

    social_proof_score = sum(dimension_score * weight)
    social_proof_tier = percentile_tier(social_proof_score, all_scores)
```

### 2.2 SeguranÃ§a (Security)

**NOTA:** SeguranÃ§a Ã© a ÃšNICA dimensÃ£o com alguns critÃ©rios absolutos, pois CVEs sÃ£o fatos objetivos.

**MÃ©tricas coletadas:**

| MÃ©trica | Fonte | Tipo |
|---------|-------|------|
| `critical_cves` | NVD/MITRE | Absoluto (0 = ideal) |
| `high_cves` | NVD/MITRE | Absoluto (0 = ideal) |
| `medium_cves` | NVD/MITRE | Linear |
| `low_cves` | NVD/MITRE | Linear |
| `deps_vulnerabilities` | npm audit / pip | Contagem |
| `has_security_policy` | GitHub | Boolean |
| `has_security_audit` | Pesquisa | Boolean |
| `signed_releases` | GitHub | Boolean |

**CÃ¡lculo do Security Score:**

```yaml
security_assessment:
  # CritÃ©rios FACTUAIS (nÃ£o arbitrÃ¡rios)
  factual_flags:
    - critical_cve_unpatched:
        condition: "CVE crÃ­tica conhecida sem patch"
        action: "FLAG - requer atenÃ§Ã£o humana"
        note: "NÃƒO Ã© VETO automÃ¡tico, mas destaque importante"

    - malware_history:
        condition: "Incidente de seguranÃ§a documentado"
        action: "FLAG - requer anÃ¡lise do incidente"
        note: "Pode ter sido resolvido, humano decide"

  # Scoring relativo (comparando com outros do dataset)
  relative_scoring:
    base: 10

    deductions_relative:
      # Comparar quantidade de CVEs com outros projetos
      - cve_count_tier:
          tier_1: -0  # Menos CVEs que 80% dos projetos
          tier_2: -1  # Menos que mediana
          tier_3: -2  # Mais que mediana
          tier_4: -3  # Mais que 80% dos projetos

      - deps_vulnerabilities_tier:
          tier_1: -0
          tier_2: -0.5
          tier_3: -1
          tier_4: -2

    bonuses:
      - has_security_policy: +0.5
      - has_security_audit: +1.0
      - signed_releases: +0.5
      - bug_bounty_program: +0.5

  final_score: "base - deductions + bonuses"
  final_tier: "percentile_tier(score, all_security_scores)"
```

### 2.3 Maturidade (Maturity)

**MÃ©tricas coletadas:**

| MÃ©trica | Fonte | Tipo |
|---------|-------|------|
| `age_months` | GitHub created_at | Linear |
| `major_version` | package.json/setup.py | Linear |
| `breaking_changes_last_year` | CHANGELOG | Inverso |
| `documentation_completeness` | Manual check | Score 1-5 |
| `has_examples` | Repo check | Boolean |
| `has_tests` | Repo check | Boolean |
| `test_coverage` | Codecov/similar | Percentual |

**CÃ¡lculo:**

```yaml
maturity_tier:
  components:
    - dimension: longevity
      metrics: [age_months, major_version]
      weight: 0.30

    - dimension: stability
      metrics: [breaking_changes_last_year]  # Inverso
      weight: 0.25

    - dimension: documentation
      metrics: [documentation_completeness, has_examples]
      weight: 0.25

    - dimension: quality
      metrics: [has_tests, test_coverage]
      weight: 0.20

  calculation: "Same as social_proof - normalize, weight, percentile"
```

### 2.4 AdequaÃ§Ã£o ao DomÃ­nio (Domain Fit)

**Esta dimensÃ£o Ã© QUALITATIVA e requer anÃ¡lise:**

```yaml
domain_fit_assessment:
  questions:
    - capability_coverage:
        question: "Quantos dos gaps identificados esta tool preenche?"
        scoring: "gaps_filled / total_gaps * 10"
        weight: 0.40

    - specificity:
        question: "A tool foi feita para este domÃ­nio ou Ã© genÃ©rica?"
        options:
          - "Feita especificamente para o domÃ­nio": 10
          - "AdaptÃ¡vel ao domÃ­nio com config": 7
          - "GenÃ©rica mas aplicÃ¡vel": 5
          - "Tangencialmente relacionada": 3
        weight: 0.30

    - integration_complexity:
        question: "QuÃ£o fÃ¡cil integrar com o squad?"
        options:
          - "Drop-in, funciona imediatamente": 10
          - "Config simples necessÃ¡ria": 8
          - "Wrapper/adapter necessÃ¡rio": 5
          - "Desenvolvimento significativo": 3
        weight: 0.30

  tier_calculation: "percentile of (capability * 0.4 + specificity * 0.3 + integration * 0.3)"
```

---

## 3. RICE FRAMEWORK (Adaptado para ComparaÃ§Ã£o Relativa)

### 3.1 Reach (Alcance)

```yaml
reach:
  definition: "ProporÃ§Ã£o de use cases do squad que a tool afeta"

  calculation: |
    reach_raw = use_cases_affected / total_use_cases
    reach_score = reach_raw * 10

  # Tier Ã© relativo aos outros tools encontrados
  tier: "percentile_tier(reach_score, all_reach_scores)"

  interpretation:
    tier_1: "Afeta mais use cases que 80% das alternativas"
    tier_2: "Afeta mais que a mediana"
    tier_3: "Afeta menos que a mediana"
    tier_4: "Afeta menos use cases que 80% das alternativas"
```

### 3.2 Impact (Impacto)

```yaml
impact:
  definition: "Magnitude do benefÃ­cio quando implementado"

  assessment_questions:
    - "Quanto trabalho manual elimina? (%)"
    - "Quanto melhora a qualidade do output? (%)"
    - "O squad consegue funcionar sem esta tool?"
    - "Esta tool Ã© um diferencial competitivo?"

  scoring:
    # Em vez de valores fixos, comparar benefÃ­cios entre tools
    calculation: |
      impact_raw = (manual_work_reduction * 0.3 +
                    quality_improvement * 0.3 +
                    necessity_score * 0.2 +
                    differentiation_score * 0.2)

      # Normalizar para escala 0.25 - 3.0 (RICE tradicional)
      impact_normalized = 0.25 + (impact_raw / 10) * 2.75

  tier: "percentile_tier(impact_normalized, all_impact_scores)"
```

### 3.3 Confidence (ConfianÃ§a)

```yaml
confidence:
  definition: "Certeza sobre as estimativas de Reach e Impact"

  evidence_levels:
    high_confidence:  # 90-100%
      criteria:
        - "Testamos a tool em ambiente similar"
        - "Temos mÃ©tricas concretas de uso"
        - "Case studies documentados no nosso contexto"

    medium_confidence:  # 70-89%
      criteria:
        - "Reviews confiÃ¡veis de usuÃ¡rios similares"
        - "DocumentaÃ§Ã£o clara com exemplos"
        - "Comunidade ativa respondendo dÃºvidas"

    low_confidence:  # 50-69%
      criteria:
        - "Parece promissor pela descriÃ§Ã£o"
        - "Pouca evidÃªncia de uso real"
        - "DocumentaÃ§Ã£o escassa"

    speculative:  # < 50%
      criteria:
        - "Projeto muito novo"
        - "Nenhuma evidÃªncia de uso"
        - "Apenas README bÃ¡sico"

  scoring: |
    # Baseado em evidÃªncias encontradas durante pesquisa
    confidence_score = evidence_points / max_possible_points * 100

  tier: "percentile_tier(confidence_score, all_confidence_scores)"
```

### 3.4 Effort (EsforÃ§o)

```yaml
effort:
  definition: "Tempo e recursos para implementar"

  components:
    installation:
      one_command: 0.1      # npm install, brew install
      few_steps: 0.25       # config file necessÃ¡rio
      moderate_setup: 0.5   # env vars, API keys
      complex_setup: 1.0    # docker, mÃºltiplas deps

    integration:
      drop_in: 0.1          # Usar diretamente
      minor_wrapper: 0.25   # Adapter simples
      moderate_code: 0.5    # Algum desenvolvimento
      significant_dev: 1.0  # IntegraÃ§Ã£o custom

    learning_curve:
      intuitive: 0.0
      quick_docs: 0.1
      tutorial_needed: 0.25
      deep_learning: 0.5

    maintenance:
      set_and_forget: 0.0
      occasional: 0.1
      monthly: 0.25
      weekly: 0.5

  calculation: |
    effort_raw = installation + integration + learning + maintenance
    # Escala 0.25 - 8 (RICE tradicional)
    effort_normalized = 0.25 + (effort_raw / 4) * 7.75

  # NOTA: Para effort, MENOR Ã© MELHOR
  # EntÃ£o tier 1 = menor effort (top 20% mais fÃ¡ceis)
  tier: "percentile_tier_inverse(effort_normalized, all_effort_scores)"
```

### 3.5 RICE Score Final

```yaml
rice_calculation:
  formula: "(Reach * Impact * Confidence) / Effort"

  # O score RICE Ã© comparado relativamente
  interpretation: |
    NÃ£o existe "RICE > 50 Ã© bom" universal.

    Comparamos RICE scores entre as ferramentas encontradas:
    - Tier 1: Top 20% (melhores relaÃ§Ã£o benefÃ­cio/esforÃ§o)
    - Tier 2: 21-50%
    - Tier 3: 51-80%
    - Tier 4: Bottom 20%
```

---

## 4. WSJF FRAMEWORK (Weighted Shortest Job First)

### 4.1 Cost of Delay

```yaml
cost_of_delay:
  components:
    user_business_value:
      weight: 0.40
      question: "Quanto valor de negÃ³cio o usuÃ¡rio ganha?"
      assessment: |
        Comparar entre as tools encontradas:
        - Qual gera mais valor imediato?
        - Qual resolve problema mais crÃ­tico?
      scoring: "relative_ranking(value_estimates) * 10"

    time_criticality:
      weight: 0.30
      question: "QuÃ£o urgente Ã© ter esta capability?"
      context_dependent: |
        Depende do squad sendo criado:
        - Squad para lanÃ§amento iminente = alta criticidade
        - Squad para exploraÃ§Ã£o = baixa criticidade
      scoring: "1-10 baseado no contexto do projeto"

    risk_reduction:
      weight: 0.30
      question: "Quanto risco a tool mitiga?"
      examples:
        - "Automatiza processo propenso a erros"
        - "Adiciona validaÃ§Ã£o/verificaÃ§Ã£o"
        - "Reduz dependÃªncia de conhecimento tÃ¡cito"
      scoring: "relative_ranking(risk_reduction) * 10"

  calculation: |
    cod = (user_value * 0.4) + (time_crit * 0.3) + (risk_red * 0.3)
```

### 4.2 Job Duration

```yaml
job_duration:
  definition: "Tempo estimado para implementar"

  # Mapeamento para escala 1-10
  mapping:
    1: "< 1 hora"
    2: "1-4 horas"
    3: "4-8 horas (1 dia)"
    4: "1-2 dias"
    5: "3-5 dias (1 semana)"
    6: "1-2 semanas"
    7: "2-4 semanas"
    8: "1-2 meses"
    9: "2-3 meses"
    10: "> 3 meses"

  estimation_factors:
    - complexity_of_integration
    - team_familiarity
    - documentation_quality
    - dependencies_required
```

### 4.3 WSJF Score

```yaml
wsjf_calculation:
  formula: "Cost_of_Delay / Job_Duration"

  interpretation: |
    WSJF alto = Alto valor, baixo esforÃ§o (fazer primeiro)
    WSJF baixo = Baixo valor ou alto esforÃ§o (fazer depois)

    ComparaÃ§Ã£o relativa entre tools encontradas determina prioridade.
```

---

## 5. COST-BENEFIT ANALYSIS (Paid vs Open Source)

### 5.1 PrincÃ­pio

```yaml
principle: |
  NÃƒO existe regra "sÃ³ paga se ROI > X%".

  A decisÃ£o depende do CONTEXTO:
  - Budget do projeto
  - Criticidade da capability
  - Qualidade das alternativas OSS encontradas
  - Tempo disponÃ­vel para integraÃ§Ã£o
```

### 5.2 AnÃ¡lise Comparativa

```yaml
paid_vs_oss_analysis:
  step_1_inventory:
    action: "Listar todas as alternativas encontradas"
    separate: "Paid vs OSS"

  step_2_capability_matrix:
    action: "Criar matriz de capabilities"
    columns: [tool_name, capability_1, capability_2, ...]
    values: [supports, partial, not_supported]

  step_3_compare_within_tiers:
    action: |
      Para cada capability gap:
      1. Identificar tools que preenchem (paid e oss)
      2. Comparar tiers de social_proof, security, maturity
      3. Comparar RICE/WSJF scores

    output:
      - "OSS Tool X Ã© Tier 1 em tudo â†’ preferir OSS"
      - "Paid Tool Y Ã© Tier 1, OSS Ã© Tier 3 â†’ considerar paid"
      - "Empate tÃ©cnico â†’ preferir OSS (custo zero)"

  step_4_cost_context:
    questions:
      - "Qual o budget disponÃ­vel para tools?"
      - "Custo da paid vs tempo economizado?"
      - "Paid tem free tier suficiente para o uso?"

    analysis: |
      Se OSS Tier 1-2 existe: Usar OSS
      Se sÃ³ Paid Ã© Tier 1 e OSS Ã© Tier 3-4:
        - Calcular: (tempo_economizado * valor_hora) vs custo_mensal
        - Apresentar anÃ¡lise para decisÃ£o humana
```

### 5.3 ApresentaÃ§Ã£o da AnÃ¡lise

```yaml
cost_benefit_output:
  format: |
    ## AnÃ¡lise: {capability_name}

    ### Alternativas Encontradas
    | Tool | Tipo | Tier Geral | Custo | Gaps Preenchidos |
    |------|------|------------|-------|------------------|
    | Tool A | OSS | Tier 2 | Free | 3/5 |
    | Tool B | Paid | Tier 1 | $49/mo | 5/5 |
    | Tool C | OSS | Tier 3 | Free | 2/5 |

    ### ComparaÃ§Ã£o Detalhada (Tier 1-2 apenas)
    | DimensÃ£o | Tool A (OSS) | Tool B (Paid) |
    |----------|--------------|---------------|
    | Social Proof | Tier 2 | Tier 1 |
    | Security | Tier 1 | Tier 1 |
    | Maturity | Tier 2 | Tier 1 |
    | Domain Fit | Tier 2 | Tier 1 |

    ### AnÃ¡lise de Custo
    - Tool B preenche 2 gaps extras (4 e 5)
    - Estimativa de tempo economizado: X horas/mÃªs
    - Break-even: Se hora vale > $Y, paid compensa

    ### RecomendaÃ§Ã£o
    [Apresentar opÃ§Ãµes para decisÃ£o humana, nÃ£o decidir automaticamente]
```

---

## 6. COMPOSITE SCORING (Tiers Finais)

### 6.1 CÃ¡lculo do Tier Composto

```yaml
composite_tier:
  weights:
    rice_tier: 0.30
    wsjf_tier: 0.25
    social_proof_tier: 0.20
    security_tier: 0.15
    maturity_tier: 0.10

  calculation: |
    # Converter tiers para scores (Tier 1 = 4, Tier 2 = 3, etc.)
    tier_to_score = {1: 4, 2: 3, 3: 2, 4: 1}

    composite_score = (
      tier_to_score[rice_tier] * 0.30 +
      tier_to_score[wsjf_tier] * 0.25 +
      tier_to_score[social_proof_tier] * 0.20 +
      tier_to_score[security_tier] * 0.15 +
      tier_to_score[maturity_tier] * 0.10
    )

    # Reconverter para tier
    if composite_score >= 3.5: return Tier 1
    elif composite_score >= 2.5: return Tier 2
    elif composite_score >= 1.5: return Tier 3
    else: return Tier 4
```

### 6.2 Decision Matrix (Relativa)

```
                    Tier 1-2 Effort      Tier 3-4 Effort
                    (Mais fÃ¡cil)         (Mais difÃ­cil)
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   Tier 1-2 Value   â”‚   QUICK WINS    â”‚   STRATEGIC     â”‚
   (Mais valor)     â”‚   Fazer agora   â”‚   Planejar      â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   Tier 3-4 Value   â”‚   FILL-INS      â”‚   BACKLOG       â”‚
   (Menos valor)    â”‚   Se sobrar     â”‚   Talvez nunca  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 7. FLAGS E DESTAQUES (Em vez de VETOs)

### 7.1 Filosofia

```yaml
no_automatic_vetos: |
  Em vez de VETO automÃ¡tico, usamos FLAGS que requerem atenÃ§Ã£o humana.

  RazÃ£o: Um projeto com "problema" pode ainda ser a melhor opÃ§Ã£o
  se todas as alternativas tiverem problemas piores.
```

### 7.2 Security Flags

```yaml
security_flags:
  - flag: "ğŸ”´ CRITICAL_CVE"
    condition: "CVE crÃ­tica nÃ£o corrigida"
    action: "Destacar no relatÃ³rio, requerer anÃ¡lise"
    note: "Verificar se hÃ¡ patch disponÃ­vel ou workaround"

  - flag: "ğŸŸ  HIGH_CVE"
    condition: "CVE alta nÃ£o corrigida"
    action: "Destacar, comparar com alternativas"

  - flag: "ğŸŸ¡ SECURITY_INCIDENT"
    condition: "HistÃ³rico de incidente de seguranÃ§a"
    action: "Pesquisar detalhes, verificar se foi resolvido"

  - flag: "ğŸŸ¢ SECURITY_AUDITED"
    condition: "Passou por auditoria formal"
    action: "Destacar positivamente"
```

### 7.3 Maturity Flags

```yaml
maturity_flags:
  - flag: "ğŸ”µ VERY_NEW"
    condition: "Projeto com < 3 meses"
    action: "Destacar, pode ser inovador ou instÃ¡vel"

  - flag: "ğŸŸ¤ SINGLE_MAINTAINER"
    condition: "Apenas 1 contribuidor"
    action: "Destacar bus factor, verificar se hÃ¡ org por trÃ¡s"

  - flag: "âšª NO_RELEASES"
    condition: "Sem releases em 12 meses"
    action: "Verificar se Ã© abandonado ou estÃ¡vel"
```

---

## 8. OUTPUT FORMAT

### 8.1 RelatÃ³rio por Tool

```yaml
tool_evaluation_report:
  tool_name: ""
  category: "mcp | api | cli | library | github"

  # MÃ©tricas brutas coletadas
  raw_metrics:
    github_stars: N
    downloads: N
    contributors: N
    last_commit: "YYYY-MM-DD"
    # ... todas as mÃ©tricas

  # Tiers relativos (comparado com outras N tools encontradas)
  relative_tiers:
    social_proof: "Tier X of 4 (top Y%)"
    security: "Tier X of 4"
    maturity: "Tier X of 4"
    domain_fit: "Tier X of 4"
    rice: "Tier X of 4"
    wsjf: "Tier X of 4"
    composite: "Tier X of 4"

  # Flags de atenÃ§Ã£o
  flags: ["ğŸ”´ CRITICAL_CVE", "ğŸ”µ VERY_NEW"]

  # PosiÃ§Ã£o no ranking geral
  ranking:
    position: "N of M tools"
    quadrant: "Quick Win | Strategic | Fill-in | Backlog"

  # ComparaÃ§Ã£o com alternativas diretas
  vs_alternatives:
    - tool: "Alternative A"
      comparison: "This tool is Tier 1 vs Tier 2"
      tradeoff: "More features but harder setup"
```

### 8.2 RelatÃ³rio Comparativo

```yaml
comparative_report:
  domain: ""
  total_tools_found: N

  # DistribuiÃ§Ã£o de tiers
  tier_distribution:
    tier_1: N tools (top 20%)
    tier_2: N tools (21-50%)
    tier_3: N tools (51-80%)
    tier_4: N tools (bottom 20%)

  # Ranking geral
  overall_ranking:
    - position: 1
      tool: "Tool A"
      composite_tier: 1
      quadrant: "Quick Win"
      key_strength: "Best domain fit"
    - position: 2
      # ...

  # Por categoria
  by_category:
    mcp:
      best: "Tool X (Tier 1)"
      alternatives: ["Tool Y (Tier 2)", "Tool Z (Tier 2)"]
    api:
      # ...

  # AnÃ¡lise de gaps
  gaps_analysis:
    - gap: "email_automation"
      best_option: "Tool A (Tier 1)"
      alternatives: ["Tool B (Tier 2)"]
      recommendation: "Tool A Ã© claramente superior"

    - gap: "competitor_analysis"
      best_options: ["Tool X (Tier 1)", "Tool Y (Tier 1)"]
      note: "Empate tÃ©cnico, apresentar ambos para decisÃ£o"
```

---

## 9. QUICK REFERENCE

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              TOOL EVALUATION QUICK REFERENCE v2.0               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  PRINCÃPIO: Comparar entre si, nÃ£o contra valores absolutos     â”‚
â”‚                                                                 â”‚
â”‚  TIERS (sempre relativos ao dataset encontrado):                â”‚
â”‚    Tier 1 = Top 20% (melhores do grupo)                         â”‚
â”‚    Tier 2 = 21-50% (acima da mediana)                           â”‚
â”‚    Tier 3 = 51-80% (abaixo da mediana)                          â”‚
â”‚    Tier 4 = Bottom 20% (piores do grupo)                        â”‚
â”‚                                                                 â”‚
â”‚  DIMENSÃ•ES:                                                     â”‚
â”‚    â–¡ Social Proof  - Popularidade relativa                      â”‚
â”‚    â–¡ Security      - Vulnerabilidades (flags, nÃ£o vetos)        â”‚
â”‚    â–¡ Maturity      - Idade e estabilidade relativas             â”‚
â”‚    â–¡ Domain Fit    - AdequaÃ§Ã£o ao caso de uso                   â”‚
â”‚    â–¡ RICE Score    - Reach*Impact*Confidence/Effort             â”‚
â”‚    â–¡ WSJF Score    - Cost of Delay / Job Duration               â”‚
â”‚                                                                 â”‚
â”‚  FLAGS (requerem atenÃ§Ã£o, nÃ£o sÃ£o vetos automÃ¡ticos):           â”‚
â”‚    ğŸ”´ CRITICAL_CVE    - Verificar se hÃ¡ patch                   â”‚
â”‚    ğŸŸ  HIGH_CVE        - Comparar com alternativas               â”‚
â”‚    ğŸŸ¡ SECURITY_INCIDENT - Verificar resoluÃ§Ã£o                   â”‚
â”‚    ğŸ”µ VERY_NEW        - Pode ser inovador ou instÃ¡vel           â”‚
â”‚    ğŸŸ¤ SINGLE_MAINTAINER - Bus factor 1                          â”‚
â”‚                                                                 â”‚
â”‚  DECISION MATRIX:                                               â”‚
â”‚    Quick Win  = Tier 1-2 Value + Tier 1-2 Effort                â”‚
â”‚    Strategic  = Tier 1-2 Value + Tier 3-4 Effort                â”‚
â”‚    Fill-in    = Tier 3-4 Value + Tier 1-2 Effort                â”‚
â”‚    Backlog    = Tier 3-4 Value + Tier 3-4 Effort                â”‚
â”‚                                                                 â”‚
â”‚  PAID vs OSS:                                                   â”‚
â”‚    Comparar tiers entre opÃ§Ãµes paid e oss                       â”‚
â”‚    Se OSS Ã© Tier 1-2: preferir OSS                              â”‚
â”‚    Se empate: preferir OSS (custo zero)                         â”‚
â”‚    Se Paid >> OSS: apresentar anÃ¡lise para decisÃ£o humana       â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

_Framework Version: 2.0_
_Last Updated: 2026-02-03_
_Philosophy: Comparative analysis over absolute thresholds_
